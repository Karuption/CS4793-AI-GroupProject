{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall plan\n",
    "  - Provide a specific article\n",
    "  - Get back a tweet that imitates \"The Telegraph\"\n",
    "\n",
    "# Steps\n",
    "  1. Get the article.\n",
    "  2. Extract the title and the content.\n",
    "  3. Create a summary of the article.\n",
    "  4. Generate a tweet that imitates \"The Telegraph\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "# Needed for jupyter notebooks to work with async tasks\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Llama-index has a really easy to use news article parser. It takes a list of urls as strings\n",
    "from llama_index.readers.web import NewsArticleReader\n",
    "\n",
    "\n",
    "article_link = \"https://www.telegraph.co.uk/health-fitness/wellbeing/sex/rachel-johnson-sex-questions-relationship-dilemmas/\"\n",
    "article = NewsArticleReader(html_to_text=True).load_data([article_link])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completed\n",
    "- llama-index generates all of the metadata and the summary of the article.\n",
    "\n",
    "## Next Steps\n",
    "- Get all the tweets from the twitter account of \"The Telegraph\"\n",
    "  - [tweet dataset](kaggle.com/datasets/shashank1558/preprocessed-twitter-tweets/data)\n",
    "- Store them in a vector database\n",
    "- Use the vector database to generate a tweet that imitates \"The Telegraph\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the documents from csv files\n",
    "\n",
    "#%pip install unstructured\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "# Text files are the pivoted csv files.\n",
    "# The dataset is only comma separated tweet content. There are no headers. \n",
    "force_embeds = False \n",
    "raw_docs = None\n",
    "if isdir(\"./chroma\") is False or force_embeds is True:\n",
    "    directory_loader = DirectoryLoader(\"tweets/\", \n",
    "        glob=\"*.txt\", \n",
    "        show_progress=True, \n",
    "        use_multithreading=True,\n",
    "    )\n",
    "    raw_docs = directory_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completed\n",
    "- Documents loaded\n",
    "\n",
    "## Next Steps\n",
    "- split the documents into smaller documents to reduce the tokens in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "if raw_docs is not None:\n",
    "    split_docs = text_splitter.split_documents(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain-chroma\n",
    "# %ollama pull nomic-embed-text\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "\n",
    "# Generate the embeddings for the documents \n",
    "embeddings = OllamaEmbeddings(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"nomic-embed-text\",\n",
    "    temperature=0.1,\n",
    "    show_progress=True,\n",
    "    num_thread=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isdir\n",
    "   \n",
    "if isdir(\"./chroma\") is False or force_embeds is True:\n",
    "    #store in a vector store\n",
    "    Chroma.from_documents(\n",
    "        documents=split_docs,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=\"./chroma\",\n",
    "    )\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=\"./chroma\", \n",
    "    embedding_function=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completed\n",
    "- Defined embedding function\n",
    "- Saved to vector database (chroma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "#    search_kwargs={\"k\":6}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"mistral\",\n",
    "    temperature=0.1,\n",
    "    num_thread=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_retrieved_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to to generate a short sentence based on the article provided.\n",
    "Keep the tweet as concise, sparse, and close to the context's provided language use as possible.\n",
    "\n",
    "{context}\n",
    "\n",
    "Article: {question}\n",
    "\n",
    "Short Sentence:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_retrieved_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'British journalist, television presenter, and author Rachel Johnson will answer your queries about sex and relationships in an upcoming Q&A.\\nAs The Telegraph’s sex and relationships agony aunt, Rachel has responded to many readers’ dilemmas in her column, including concerns about marital affairs; queries about threesomes; intimacy issues; and how to be more adventurous in bed – and is keen to help more readers.\\nIf you’ve got a burning question you’d like answered, submit it via the form below.\\nIf you’d prefer to remain anonymous, simply tick the checkbox.'"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article[0].metadata['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:03<00:00,  3.44s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Rachel Johnson, a British journalist, television presenter, and author, will answer your sex and relationship questions in an upcoming Q&A as The Telegraph's agony aunt. Submit your burning question via the form below for her response, with the option to remain anonymous if preferred.\""
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request = \"Match the context in vocabulary, writing style, and tone for the following content:\\n\\n\" + str(article[0].metadata['summary'])\n",
    "\n",
    "rag_chain.invoke(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Rachel Johnson, known for her work as a British journalist, television presenter, and author, will answer anonymous questions about sex and relationships in an upcoming Q&A session for The Telegraph.'"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Generate a short sentence that generally describes the given: \\n\\n\" + str(article[0].metadata['summary']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
